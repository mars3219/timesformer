{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyixuxu/TimeSformer/blob/main/visualizing_space_time_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkMqF4RNdboe"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK1TO5NAiq8-"
      },
      "source": [
        "install packages required for TimesFormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMmwHCkSdAxG",
        "outputId": "0b35583f-d839-45ca-aaf1-a2117c13c558"
      },
      "outputs": [],
      "source": [
        "# # install packages needed for TimeSformer\n",
        "# ! pip install torchvision\n",
        "# ! pip install 'git+https://github.com/facebookresearch/fvcore'\n",
        "# ! pip install simplejson\n",
        "# ! pip install einops\n",
        "# ! pip install timm\n",
        "# ! pip install psutil\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install opencv-python\n",
        "# ! pip install tensorboard\n",
        "# ! pip install av"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ258yiTjFiI"
      },
      "source": [
        "install TimeSformer \n",
        "\n",
        "* we are using a version I forked - where I added some example data and fixed a few import issue so we can use with colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vEz3aJfX8xo",
        "outputId": "cc30ce07-f816-48b7-e24e-b34ed93058a6"
      },
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/yiyixuxu/TimeSformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0CDM7bXc4Vy",
        "outputId": "f31f2d82-ccd3-4e66-a059-cc075e3efc53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'TimeSformer'\n",
            "/workspace/TimeSformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/env/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        }
      ],
      "source": [
        "%cd TimeSformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HQtop8Xr48HK"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "from pathlib import Path\n",
        "from timesformer.models.vit import *\n",
        "from timesformer.datasets import utils as utils\n",
        "from timesformer.config.defaults import get_cfg\n",
        "from einops import rearrange, repeat, reduce\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiTDjCG6wf3x"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N8UIk_MFwn5M"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "DEFAULT_MEAN = [0.45, 0.45, 0.45]\n",
        "DEFAULT_STD = [0.225, 0.225, 0.225]\n",
        "# convert video path to input tensor for model\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(DEFAULT_MEAN,DEFAULT_STD),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "])\n",
        "\n",
        "# convert the video path to input for cv2_imshow()\n",
        "transform_plot = transforms.Compose([\n",
        "    lambda p: cv2.imread(str(p),cv2.IMREAD_COLOR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    lambda x: rearrange(x*255, 'c h w -> h w c').numpy()\n",
        "])\n",
        "\n",
        "\n",
        "def get_frames(path_to_video, num_frames=8):\n",
        "  \"return a list of paths to the frames of sampled from the video\"\n",
        "  path_to_frames = list(path_to_video.iterdir())\n",
        "  path_to_frames.sort(key=lambda f: int(f.with_suffix('').name[-6:]))\n",
        "  assert num_frames <= len(path_to_frames), \"num_frames can't exceed the number of frames extracted from videos\"\n",
        "  if len(path_to_frames) == num_frames:\n",
        "    return(path_to_frames)\n",
        "  else:\n",
        "    video_length = len(path_to_frames)\n",
        "    seg_size = float(video_length - 1) / num_frames \n",
        "    seq = []\n",
        "    for i in range(num_frames):\n",
        "      start = int(np.round(seg_size * i))\n",
        "      end = int(np.round(seg_size * (i + 1)))\n",
        "      seq.append((start + end) // 2)\n",
        "      path_to_frames_new = [path_to_frames[p] for p in seq]\n",
        "    return(path_to_frames_new)\n",
        "\n",
        "def create_video_input(path_to_video):\n",
        "  \"create the input tensor for TimeSformer model\"\n",
        "  path_to_frames = get_frames(path_to_video)\n",
        "  frames = [transform(cv2.imread(str(p), cv2.IMREAD_COLOR)) for p in path_to_frames]\n",
        "  frames = torch.stack(frames, dim=0)\n",
        "  frames = rearrange(frames, 't c h w -> c t h w')\n",
        "  frames = frames.unsqueeze(dim=0)\n",
        "  return(frames)\n",
        "\n",
        "def show_mask_on_image(img, mask):\n",
        "    img = np.float32(img) / 255\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return np.uint8(255 * cam)\n",
        "\n",
        "\n",
        "def create_masks(masks_in, np_imgs):\n",
        "  masks = []\n",
        "  for mask, img in zip(masks_in, np_imgs):\n",
        "    mask= cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "    mask = show_mask_on_image(img, mask)\n",
        "    masks.append(mask)\n",
        "  return(masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cKHAX0VhSXKX"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "def combine_divided_attention(attn_t, attn_s):\n",
        "  ## time attention\n",
        "    # average time attention weights across heads\n",
        "  attn_t = attn_t.mean(dim = 1)\n",
        "    # add cls_token to attn_t as an identity matrix since it only attends to itself \n",
        "  I = torch.eye(attn_t.size(-1)).unsqueeze(0)\n",
        "  attn_t = torch.cat([I,attn_t], 0)\n",
        "    # adding identity matrix to account for skipped connection \n",
        "  attn_t = attn_t +  torch.eye(attn_t.size(-1))[None,...]\n",
        "    # renormalize\n",
        "  attn_t = attn_t / attn_t.sum(-1)[...,None]\n",
        "\n",
        "  ## space attention\n",
        "   # average across heads\n",
        "  attn_s = attn_s.mean(dim = 1)\n",
        "   # adding residual and renormalize \n",
        "  attn_s = attn_s +  torch.eye(attn_s.size(-1))[None,...]\n",
        "  attn_s = attn_s / attn_s.sum(-1)[...,None]\n",
        "  \n",
        "  ## combine the space and time attention\n",
        "  attn_ts = einsum('tpk, ktq -> ptkq', attn_s, attn_t)\n",
        "  \n",
        "  ## average the cls_token attention across the frames\n",
        "   # splice out the attention for cls_token\n",
        "  attn_cls = attn_ts[0,:,:,:]\n",
        "   # average the cls_token attention and repeat across the frames\n",
        "  attn_cls_a = attn_cls.mean(dim=0)\n",
        "  attn_cls_a = repeat(attn_cls_a, 'p t -> j p t', j = 8)\n",
        "   # add it back\n",
        "  attn_ts = torch.cat([attn_cls_a.unsqueeze(0),attn_ts[1:,:,:,:]],0)\n",
        "  return(attn_ts)\n",
        "\n",
        "class DividedAttentionRollout():\n",
        "  def __init__(self, model, **kwargs):\n",
        "    self.model = model\n",
        "    self.hooks = []\n",
        "\n",
        "  def get_attn_t(self, module, input, output):\n",
        "    self.time_attentions.append(output.detach().cpu())\n",
        "  def get_attn_s(self, module, input, output):\n",
        "    self.space_attentions.append(output.detach().cpu())\n",
        "\n",
        "  def remove_hooks(self): \n",
        "    for h in self.hooks: h.remove()\n",
        "    \n",
        "  def __call__(self, path_to_video):\n",
        "    input_tensor = create_video_input(path_to_video)\n",
        "    self.model.zero_grad()\n",
        "    self.time_attentions = []\n",
        "    self.space_attentions = []\n",
        "    self.attentions = []\n",
        "    for name, m in model.named_modules():\n",
        "      if 'temporal_attn.attn_drop' in name:\n",
        "        self.hooks.append(m.register_forward_hook(self.get_attn_t))\n",
        "      elif 'attn.attn_drop' in name:\n",
        "        self.hooks.append(m.register_forward_hook(self.get_attn_s))\n",
        "    preds = self.model(input_tensor)\n",
        "    for h in self.hooks: h.remove()\n",
        "    for attn_t,attn_s in zip(self.time_attentions, self.space_attentions):\n",
        "      self.attentions.append(combine_divided_attention(attn_t,attn_s))\n",
        "    p,t = self.attentions[0].shape[0], self.attentions[0].shape[1]\n",
        "    result = torch.eye(p*t)\n",
        "    for attention in self.attentions:\n",
        "      attention = rearrange(attention, 'p1 t1 p2 t2 -> (p1 t1) (p2 t2)')\n",
        "      result = torch.matmul(attention, result)\n",
        "    mask = rearrange(result, '(p1 t1) (p2 t2) -> p1 t1 p2 t2', p1 = p, p2=p)\n",
        "    mask = mask.mean(dim=1)\n",
        "    mask = mask[0,1:,:]\n",
        "    width = int(mask.size(0)**0.5)\n",
        "    mask = rearrange(mask, '(h w) t -> h w t', w = width).numpy()\n",
        "    mask = mask / np.max(mask)\n",
        "    return(mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H20-Np38qhVM"
      },
      "source": [
        "# load the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3L75p3W-mLn"
      },
      "source": [
        "download the pre-trainde model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip6HavtbtmRh",
        "outputId": "2d233350-2589-4c90-9545-22cd66fedb5c"
      },
      "outputs": [],
      "source": [
        "# ! wget https://dl.dropboxusercontent.com/s/tybhuml57y24wpm/TimeSformer_divST_8_224_SSv2.pyth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H1oq7vF-pxD"
      },
      "source": [
        "load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORsqV7ZlsEJB",
        "outputId": "5c8d609d-ad53-4a61-cb51-fa912ffa2d77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_file = '/workspace/TimeSformer/TimeSformer_divST_8_224_SSv2.pyth'\n",
        "Path(model_file).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HskcnYcS1jSo"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file('/workspace/TimeSformer/configs/SSv2/TimeSformer_divST_8_224.yaml')\n",
        "cfg.TRAIN.ENABLE = False\n",
        "cfg.TIMESFORMER.PRETRAINED_MODEL = model_file\n",
        "model = MODEL_REGISTRY.get('vit_base_patch16_224')(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_a7HTsI-2DI"
      },
      "source": [
        "read the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y4AvpeqOjk2D"
      },
      "outputs": [],
      "source": [
        "with open('/workspace/TimeSformer/example_data/labels.json') as f:\n",
        "  ssv2_labels = json.load(f)\n",
        "ssv2_labels = list(ssv2_labels.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Hia6Ph-51o"
      },
      "source": [
        "inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9G1vctJ7kxv",
        "outputId": "9ccf414f-3751-4fa8-8f9b-22ed70d7824f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path_to_video = Path('/workspace/TimeSformer/example_data/74225/')\n",
        "path_to_video.exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gyojK4Iy7QFL"
      },
      "outputs": [],
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "  np.random.seed(cfg.RNG_SEED)\n",
        "  torch.manual_seed(cfg.RNG_SEED)\n",
        "  model.eval()\n",
        "  pred = model(create_video_input(path_to_video)).cpu().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEphQOJE8NnU",
        "outputId": "6e61af10-247c-4e36-bfb4-e26d0d367704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction index 0: Spinning something that quickly stops spinning, score: 10.782\n",
            "Prediction index 1: Pushing something so it spins, score: 6.669\n",
            "Prediction index 2: Moving something and something away from each other, score: 6.461\n",
            "Prediction index 3: Tipping something over   , score: 5.723\n",
            "Prediction index 4: Pretending to turn something upside down, score: 5.554\n"
          ]
        }
      ],
      "source": [
        "topk_scores, topk_label = torch.topk(pred, k=5, dim=-1)\n",
        "for i in range(5):\n",
        "  pred_name = ssv2_labels[topk_label.squeeze()[i].item()]\n",
        "  print(f\"Prediction index {i}: {pred_name:<25}, score: {topk_scores.squeeze()[i].item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGmROAg9pOO"
      },
      "source": [
        "# visualizing the learned space-time attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci8UT8yB_ErR"
      },
      "source": [
        "Create a `DividedAttentionRollout` object (`att_roll`) and call it to get a mask for a given video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l3qCY_CS9rL-"
      },
      "outputs": [],
      "source": [
        "att_roll = DividedAttentionRollout(model)\n",
        "masks = att_roll(path_to_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dfas1gR_VM_"
      },
      "source": [
        "plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "2142VN54-Gvb",
        "outputId": "122f6cd6-6ea3-406b-e739-2a4faf6d64f5"
      },
      "outputs": [],
      "source": [
        "np_imgs = [transform_plot(p) for p in get_frames(path_to_video)]\n",
        "masks = create_masks(list(rearrange(masks, 'h w t -> t h w')),np_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(224, 1792, 3)\n"
          ]
        }
      ],
      "source": [
        "stacked_img = np.hstack(np_imgs)\n",
        "print(stacked_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv2.imshow('show1', stacked_img)\n",
        "# cv2.imshow(np.hstack(masks))\n",
        "cv2.waitKey(10)\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOtDwQEa7MFcIjh+elUfcY5",
      "collapsed_sections": [
        "H20-Np38qhVM"
      ],
      "include_colab_link": true,
      "name": "TimeSformer_rolled_attention.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
